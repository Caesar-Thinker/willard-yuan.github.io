---
layout: post
title: 数据分类浅谈：决策数
categories: [机器学习]
---

## 背景
决策树（decision tree）是一种基本的分类和**回归**（后面补充一个回归的例子？）方法，它呈现的是一种树形结构，可以认为是if-then规则的集合。其其主要优点是模型具有很好的**可读性**，且**分类速度快**；缺点是可能会产生过度匹配的问题（所以一般都会有决策树的剪枝过程）。决策树在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型，其学习过程包括3个步骤：特征选择、决策树生成和决策树修剪。决策树学习的思想来源主要有Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及由Breiman等人在1986年提出的CART算法。

决策树在现实生活中，已得到了很广泛的应用，举两个很常见的例子：

1. 猜测人物，游戏的规则很简单：参与游戏的一方在脑海里想某个人，然后游戏页面一次一个选择选项陆续让你回答，比如，“这个人是在现代还是古代？”，一般问题的答案只能回答是还是不是，最后根据你一系列的回答便可以对你所想的某个人做出推断，比如你所想的这个人是你老爸。

2. 投资指导或贷款申请。比如炒股，通过app新开账户后，有的会给出一个投资建议的指导，在生成这份指导建议前，会让你对某些问题（一般几个）进行回答，然后根据你的回答，生成一份对你投资类型的判断，开过这类账户的用户应该都有这个体验。另外一个就是贷款申请，会根据你的年龄、有无工作、是否有房子、信贷情况指标来决定是否给你贷款。

实际上，近来的调查研究表明决策树也是**最经常**使用的数据挖掘算法。接下来，按决策树的学习步骤先介绍决策树模型定义，然后介绍特征选择、决策树生成以及决策树的剪枝，最后编码测试一个实际的例子。

## 决策树模型定义

决策树模型是一种描述对实例进行分类的树形结构，它有结点（node）和有向边（directed edge）组成，结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或者属性，叶结点表示一个类别。

在用决策树进行预测时，对于输入的某个样本$x_{test}$，从根结点开始，对$x_{test}$的某一个特征进行测试，根据测试的结果，将$x_{test}$分配分配到其子结点；然后递归的进行下一个规则的判断，直到到达叶结点，最后将$x_{test}$分配到叶结点的类中。

下图是决策树的示意图，图中的方框表示内部结点，圆型表示叶结点，即所属的类别。
![]()
再举个具体的例子：
![]()
上面，将邮件按重要性分成了3类：即无聊时阅读的邮件、需要即使处理的邮件以及无需阅读的垃圾邮件。

在前面**背景**中讲到，可以认为决策树是if-then规则的集合。从上面给出的两个图示也可以很明显的得到这样的结论。决策树转换成if-then规则的过程是这样的：由决策树的根结点的每一条路径构建一条规则，路径上内部结点的特征对应着规则的条件，叶结点对应的是规则的结论。由于决策树在对测试样本进行类别判定时，经过的判别路径（规则）只有一条，且判定的结果一定是属于叶结点中的某个，所以可以得出一个重要的性质：决策树的路径是**完备且互斥**的。也就是每一个实例都只能被一条路径（对应的规则子集）所覆盖（或满足）。

## 决策树学习

对于给定的数据集
$$
D={\{(x_1, y_1),(x_2, y_2),...,(x_N, y_N) \}}
$$
N个样本，$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$为样本$i$对应的$n$个特征，比如前面所举的贷款的例子，年龄、有无工作、是否有房子、信贷情况共4个特征，$y\in{\{1,2,...,K\}}$为类标记，决策树学习的目标就是要根据给定的训练集构造一个决策树模型，是它能对未知样本进行正确的分类。




